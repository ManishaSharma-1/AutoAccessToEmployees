{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "x = pandas.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import plotly\n",
    "\n",
    "x_val = ['Action Denied','Action Accepted']\n",
    "colors = ['lightslategray','crimson'] \n",
    "\n",
    "x_0 = len(x[x['ACTION'] == 0])\n",
    "x_1 = len(x[x['ACTION'] == 1])\n",
    "fig = plotly.graph_objects.Figure([go.Bar(x=x_val, y = [x_0,x_1], text=[x_0,x_1],textposition='auto',  marker_color=colors)])\n",
    "fig.update_layout(title_text='Count of Action Denied and Action Accepted - Clearly Class Imbalance!')\n",
    "a = fig.to_json()\n",
    "z = json.dumps(a,cls = plotly.utils.PlotlyJSONEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = pandas.DataFrame([(col,x[col].nunique()) for col in x.columns], \n",
    "                           columns=['Columns', 'Count of Category'])\n",
    "colors = ['lightslategray'] \n",
    "\n",
    "fig = plotly.graph_objects.Figure([go.Bar(x=unique.Columns, y = unique['Count of Category'], text=unique['Count of Category'],textposition='auto',  marker_color=colors)])\n",
    "fig.update_layout(title_text='Unique categories - Train Data Set!')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fig.to_json()\n",
    "z = json.dumps(a,cls = plotly.utils.PlotlyJSONEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Duplicate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "target = \"ACTION\"\n",
    "col4train = [val for val in train.columns if val!=target]\n",
    "\n",
    "col1 = 'ROLE_CODE'\n",
    "col2 = 'ROLE_TITLE'\n",
    "\n",
    "pair = len(train.groupby([col1,col2]).size())\n",
    "single = len(train.groupby([col1]).size())\n",
    "\n",
    "print(col1, col2, pair, single)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(x.ROLE_TITLE.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = set(x.ROLE_CODE.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.intersection(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in a:\n",
    "    if i in b:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = x[['ROLE_CODE', 'ROLE_TITLE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotlist = []\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "fig = sns.heatmap(df.corr(),annot=True, cmap ='viridis', linewidth = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(plotlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = []\n",
    "fig = plt.figure()\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(1,10):\n",
    "    fig =plt.subplot(5,2,i)\n",
    "    fig = plt.hist(df[df.columns[i]])\n",
    "    fig = plt.xlabel(df.columns[i])\n",
    "    fig = plt.ylabel(\"Frequency\")\n",
    "buf = io.BytesIO()\n",
    "fig.figure.savefig(buf, format = \"png\", dpi = 600, box_inches = \"tight\", pad_inches = 0)\n",
    "plotlist.append(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "X = data.iloc[:,1:]\n",
    "y = data.iloc[:,:1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) \n",
    "  \n",
    "# describes info about train and test set \n",
    "print(\"Number transactions X_train dataset: \", X_train.shape) \n",
    "print(\"Number transactions y_train dataset: \", y_train.shape) \n",
    "\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape) \n",
    "print(\"Number transactions y_test dataset: \", y_test.shape) \n",
    "\n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "\n",
    "print('before OverSampling, the shape of train_X: {}'.format(X_train.shape)) \n",
    "print('before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape)) \n",
    "  \n",
    "print(\"before OverSampling, counts of label '1': {}\".format(sum(y_train.values.ravel() == 1))) \n",
    "print(\"before OverSampling, counts of label '0': {}\".format(sum(y_train.values.ravel() == 0))) \n",
    "\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ACTION'\n",
    "train_columns = [x for x in data if x not in [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame(X_train_res, y_train_res)\n",
    "df_new.columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ACTION'\n",
    "train_columns = [x for x in X_train if x not in [target]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_MGR = df_new.groupby('ROLE_TITLE')['RESOURCE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "data = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data =amazon()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['RESOURCE'] = train_data['RESOURCE'].astype('category')\n",
    "train_data['MGR_ID'] = train_data['MGR_ID'].astype('category')\n",
    "train_data['ROLE_ROLLUP_1'] = train_data['ROLE_ROLLUP_1'].astype('category')\n",
    "train_data['ROLE_ROLLUP_2'] = train_data['ROLE_ROLLUP_2'].astype('category')\n",
    "train_data['ROLE_DEPTNAME'] = train_data['ROLE_DEPTNAME'].astype('category')\n",
    "train_data['ROLE_FAMILY_DESC'] = train_data['ROLE_FAMILY_DESC'].astype('category')\n",
    "train_data['ROLE_FAMILY'] = train_data['ROLE_FAMILY'].astype('category')\n",
    "train_data['ROLE_TITLE'] = train_data['ROLE_TITLE'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_data.groupby('ROLE_TITLE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data =amazon()\n",
    "train_data = train_data.drop(columns = 'ROLE_CODE')\n",
    "train_data['RESOURCE'] = train_data['RESOURCE'].astype('category')\n",
    "train_data['MGR_ID'] = train_data['MGR_ID'].astype('category')\n",
    "train_data['ROLE_ROLLUP_1'] = train_data['ROLE_ROLLUP_1'].astype('category')\n",
    "train_data['ROLE_ROLLUP_2'] = train_data['ROLE_ROLLUP_2'].astype('category')\n",
    "train_data['ROLE_DEPTNAME'] = train_data['ROLE_DEPTNAME'].astype('category')\n",
    "train_data['ROLE_FAMILY_DESC'] = train_data['ROLE_FAMILY_DESC'].astype('category')\n",
    "train_data['ROLE_FAMILY'] = train_data['ROLE_FAMILY'].astype('category')\n",
    "train_data['ROLE_TITLE'] = train_data['ROLE_TITLE'].astype('category')\n",
    "dfa = pd.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    df_new1 = pd.get_dummies(train_data[1][i])\n",
    "    dfa = dfa.append(df_new1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    df_new1 = pd.get_dummies(train_data[1][i])\n",
    "    dfa = dfa.append(df_new1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = dfa.drop(columns = 'ROLE_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_new1.append(df_new2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear - OHE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=True, dtype=np.float32, handle_unknown='ignore')\n",
    "X = ohe.fit_transform(df_new[train_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = train_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_new1 = pd.DataFrame()\n",
    "df_new1['RESOURCE'] = df_new['RESOURCE'].astype('category')\n",
    "df_new1['MGR_ID'] = df_new['MGR_ID'].astype('category')\n",
    "df_new1['ROLE_ROLLUP_1'] = df_new['ROLE_ROLLUP_1'].astype('category')\n",
    "df_new1['ROLE_ROLLUP_2'] = df_new['ROLE_ROLLUP_2'].astype('category')\n",
    "df_new1['ROLE_DEPTNAME'] = df_new['ROLE_DEPTNAME'].astype('category')\n",
    "df_new1['ROLE_TITLE'] = df_new['ROLE_TITLE'].astype('category')\n",
    "df_new1['ROLE_FAMILY_DESC'] = df_new['ROLE_FAMILY_DESC'].astype('category')\n",
    "df_new1['ROLE_FAMILY'] = df_new['ROLE_FAMILY'].astype('category')\n",
    "df_new1 = pd.get_dummies(df_new1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby('MGR_ID_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_7 = CatBoostClassifier(**params)\n",
    "\n",
    "cbc_7.get_feature_importance(prettified=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6));\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df);\n",
    "plt.title('CatBoost features importance:');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.iloc[:,1:]\n",
    "y = train_data.iloc[:,:1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) \n",
    "  \n",
    "# describes info about train and test set \n",
    "print(\"Number transactions X_train dataset: \", X_train.shape) \n",
    "print(\"Number transactions y_train dataset: \", y_train.shape) \n",
    "\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape) \n",
    "print(\"Number transactions y_test dataset: \", y_test.shape) \n",
    "\n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "\n",
    "print('before OverSampling, the shape of train_X: {}'.format(X_train.shape)) \n",
    "print('before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape)) \n",
    "  \n",
    "print(\"before OverSampling, counts of label '1': {}\".format(sum(y_train.values.ravel() == 1))) \n",
    "print(\"before OverSampling, counts of label '0': {}\".format(sum(y_train.values.ravel() == 0))) \n",
    "\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data =amazon()\n",
    "train_data = train_data.drop(columns = 'ROLE_CODE')\n",
    "X = train_data.iloc[:,1:]\n",
    "y = train_data.iloc[:,:1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted_labels = model.predict(X_test)\n",
    "print(\"FINISHED classifying. accuracy score : \")\n",
    "print(accuracy_score(y_test, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted_labels)\n",
    "\n",
    "disp = plot_precision_recall_curve(model, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "z = [[tn,fp],[fn,tp]]\n",
    "\n",
    "x = [1,0]\n",
    "y = [1,0]\n",
    "\n",
    "z_text  = [['True Negative : ' + str(tn),'False Positive : ' + str(fp)],['False Negative : ' + str(fn),'True Positive : ' + str(tp)]]\n",
    "\n",
    "# set up figure \n",
    "fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "# add title\n",
    "fig.update_layout(title_text='<i><b>Confusion matrix - Random Forest</b></i>',\n",
    "                 )\n",
    "\n",
    "# add custom xaxis title\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted value\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# add custom yaxis title\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.1,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Real value\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# adjust margins to make room for yaxis title\n",
    "fig.update_layout(margin=dict(t=50, l=200))\n",
    "\n",
    "# add colorbar\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "precision, recall, _ = precision_recall_curve(y_test,\n",
    "                                                    predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, s = precision_recall_fscore_support(y_test, predicted_labels, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "predicted_labels = model.predict(X_test)\n",
    "print(\"FINISHED classifying. accuracy score : \")\n",
    "print(accuracy_score(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "\n",
    "\n",
    "z = [[tn,fp],[fn,tp]]\n",
    "\n",
    "x = [1,0]\n",
    "y = [1,0]\n",
    "\n",
    "z_text  = [['True Negative : ' + str(tn),'False Positive : ' + str(fp)],['False Negative : ' + str(fn),'True Positive : ' + str(tp)]]\n",
    "\n",
    "# set up figure \n",
    "fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "# add title\n",
    "fig.update_layout(title_text='<i><b>Confusion matrix - Random Forest</b></i>',\n",
    "                 )\n",
    "\n",
    "# add custom xaxis title\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted value\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# add custom yaxis title\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.1,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Real value\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# adjust margins to make room for yaxis title\n",
    "fig.update_layout(margin=dict(t=50, l=200))\n",
    "\n",
    "# add colorbar\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "\n",
    "average_precision = average_precision_score(y_test, predicted_labels)\n",
    "\n",
    "disp = plot_precision_recall_curve(model, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "X = train_data.iloc[:,1:]\n",
    "y = train_data.iloc[:,:1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) \n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "importance = model.feature_importances_\n",
    "data1 = pd.DataFrame(importance, columns = ['Importance'])\n",
    "data1['Columns'] = train_data.iloc[:,1:].columns\n",
    "data1 = data1.sort_values('Importance')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure([go.Bar(x=data1['Columns'], y=data1['Importance'])])\n",
    "fig.update_layout(title_text='Feature Importance')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new1 = df_new1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories='auto')\n",
    "feature_arr = ohe.fit_transform(df_new[['RESOURCE','MGR_ID']]).toarray()\n",
    "feature_labels = ohe.categories_\n",
    "feature_labels = np.array(feature_labels).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(feature_arr, columns=feature_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "feature_arr = pd.DataFrame(ohe.fit_transform(df_new[['RESOURCE','MGR_ID']]), columns=df_new.columns)\n",
    "feature_labels = np.array(feature_labels).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(feature_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and one hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\manis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\manis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\manis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\manis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\manis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.datasets import amazon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data =amazon()\n",
    "train_data = train_data.drop(columns = 'ROLE_CODE')\n",
    "train_data['RESOURCE'] = train_data['RESOURCE'].astype('category')\n",
    "train_data['MGR_ID'] = train_data['MGR_ID'].astype('category')\n",
    "train_data['ROLE_ROLLUP_1'] = train_data['ROLE_ROLLUP_1'].astype('category')\n",
    "train_data['ROLE_ROLLUP_2'] = train_data['ROLE_ROLLUP_2'].astype('category')\n",
    "train_data['ROLE_DEPTNAME'] = train_data['ROLE_DEPTNAME'].astype('category')\n",
    "train_data['ROLE_FAMILY_DESC'] = train_data['ROLE_FAMILY_DESC'].astype('category')\n",
    "train_data['ROLE_FAMILY'] = train_data['ROLE_FAMILY'].astype('category')\n",
    "train_data['ROLE_TITLE'] = train_data['ROLE_TITLE'].astype('category')\n",
    "df = pandas.DataFrame(train_data.groupby('ROLE_TITLE'))\n",
    "dfa = pandas.DataFrame()\n",
    "for i in range(len(df)):\n",
    "    df_new1 = pandas.get_dummies(df[1][i])\n",
    "    dfa = dfa.append(df_new1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.to_csv(\"GroupedandOneHotEncoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.to_csv(\"GroupedandOneHotEncoded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res.to_csv(\"Sampled_X_train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (22938, 15283)\n",
      "Number transactions y_train dataset:  (22938, 1)\n",
      "Number transactions X_test dataset:  (9831, 15283)\n",
      "Number transactions y_test dataset:  (9831, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before OverSampling, the shape of train_X: (22938, 15283)\n",
      "before OverSampling, the shape of train_y: (22938, 1) \n",
      "\n",
      "before OverSampling, counts of label '1': 21569\n",
      "before OverSampling, counts of label '0': 1369\n",
      "After OverSampling, the shape of train_X: (43138, 15283)\n",
      "After OverSampling, the shape of train_y: (43138,) \n",
      "\n",
      "After OverSampling, counts of label '1': 21569\n",
      "After OverSampling, counts of label '0': 21569\n"
     ]
    }
   ],
   "source": [
    "X = dfa.iloc[:,1:]\n",
    "y = dfa.iloc[:,:1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) \n",
    "  \n",
    "# describes info about train and test set \n",
    "print(\"Number transactions X_train dataset: \", X_train.shape) \n",
    "print(\"Number transactions y_train dataset: \", y_train.shape) \n",
    "\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape) \n",
    "print(\"Number transactions y_test dataset: \", y_test.shape) \n",
    "\n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.values.ravel()) \n",
    "\n",
    "print('before OverSampling, the shape of train_X: {}'.format(X_train.shape)) \n",
    "print('before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape)) \n",
    "  \n",
    "print(\"before OverSampling, counts of label '1': {}\".format(sum(y_train.values.ravel() == 1))) \n",
    "print(\"before OverSampling, counts of label '0': {}\".format(sum(y_train.values.ravel() == 0))) \n",
    "\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "import plotly.figure_factory as ff\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = test_data.iloc[:1]\n",
    "td = td.drop(columns = 'ROLE_CODE')\n",
    "td['RESOURCE'] = td['RESOURCE'].astype('category')\n",
    "td['MGR_ID'] = td['MGR_ID'].astype('category')\n",
    "td['ROLE_ROLLUP_1'] = td['ROLE_ROLLUP_1'].astype('category')\n",
    "td['ROLE_ROLLUP_2'] = td['ROLE_ROLLUP_2'].astype('category')\n",
    "td['ROLE_DEPTNAME'] = td['ROLE_DEPTNAME'].astype('category')\n",
    "td['ROLE_FAMILY_DESC'] = td['ROLE_FAMILY_DESC'].astype('category')\n",
    "td['ROLE_FAMILY'] = td['ROLE_FAMILY'].astype('category')\n",
    "td['ROLE_TITLE'] = td['ROLE_TITLE'].astype('category')\n",
    "df = pandas.DataFrame(train_data.groupby('ROLE_TITLE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pandas.get_dummies(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.zeros(len(X_test.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handling New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.zeros(len(X_test.iloc[0]))\n",
    "new = pandas.DataFrame(new_data) \n",
    "new = new.transpose()\n",
    "new.columns = X_test.columns\n",
    "indexx = dfs.columns\n",
    "X_test = X_test.fillna(0)\n",
    "for ind in indexx:\n",
    "    if ind in X_test.columns:\n",
    "        X_test.iloc[0][ind] = 1\n",
    "\n",
    "X_test = X_test.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "predicted_labels , tn, fp, fn, tp ,plotlist= evaluate_model(X_train_res,y_train_res, model, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"plotlist_logistic.txt\", \"w\")\n",
    "text_file.write(str(plotlist[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('plotlist_logistic.py', 'w' )\n",
    "f.write( 'value = ' + plotlist[0] + '\\n' )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"plotlist_logistic\", 'wb') as output:\n",
    "    out = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outputfilename, 'wb') as output:\n",
    "    output.write(bytearray(int(i, 16) for i in yoursequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(X_train_res).to_csv(\"y_train_res.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_train_res_sampled.csv\", X_train_res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"y_train_res_sampled.csv\", y_train_res, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_test_sampled.csv\", X_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"y_test_res_sampled.csv\", y_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train_res,y_train_res, model, X_test, y_test):\n",
    "    plotlist = []\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    predicted_labels = model.predict(X_test)\n",
    "    average_precision = average_precision_score(y_test, predicted_labels)\n",
    "    tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    average_precision = average_precision_score(y_test, predicted_labels)\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "\n",
    "    fpr[2], tpr[2], _ = roc_curve(y_test, predicted_labels)\n",
    "    roc_auc[2] = auc(fpr[2], tpr[2])\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    fig = plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "    fig = plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    fig = plt.xlim([0.0, 1.0])\n",
    "    fig = plt.ylim([0.0, 1.05])\n",
    "    fig = plt.xlabel('False Positive Rate')\n",
    "    fig = plt.ylabel('True Positive Rate')\n",
    "    fig = plt.title('Receiver operating characteristic example')\n",
    "    fig = plt.legend(loc=\"lower right\")\n",
    "\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    fig.figure.savefig(buf, format = \"png\", dpi = 600, box_inches = \"tight\", pad_inches = 0)\n",
    "    plotlist.append(buf.getvalue())\n",
    "\n",
    "    return predicted_labels, tn, fp, fn, tp, plotlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotlist = []\n",
    "model.fit(X_train_res, y_train_res)\n",
    "predicted_labels = model.predict(X_test)\n",
    "average_precision = average_precision_score(y_test, predicted_labels)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "average_precision = average_precision_score(y_test, predicted_labels)\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "\n",
    "fpr[2], tpr[2], _ = roc_curve(y_test, predicted_labels)\n",
    "roc_auc[2] = auc(fpr[2], tpr[2])\n",
    "\n",
    "fig = plt.figure()\n",
    "lw = 2\n",
    "fig = plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "fig = plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "fig = plt.xlim([0.0, 1.0])\n",
    "fig = plt.ylim([0.0, 1.05])\n",
    "fig = plt.xlabel('False Positive Rate')\n",
    "fig = plt.ylabel('True Positive Rate')\n",
    "fig = plt.title('Receiver operating characteristic example')\n",
    "fig = plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "buf = io.BytesIO()\n",
    "fig.figure.savefig(buf, format = \"png\", dpi = 600, box_inches = \"tight\", pad_inches = 0)\n",
    "plotlist.append(buf.getvalue())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas.DataFrame(tpr[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fppp = pandas.DataFrame(fpr[2]).to_csv(\"FPRLogisticRegression.csv\")\n",
    "tppp = pandas.DataFrame(tpr[2]).to_csv(\"TPRLogisticRegression.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "\n",
    "abc = pandas.DataFrame([tn,fp,fn,tp], index=[\"tn\",\"fp\",\"fn\",\"tp\"])\n",
    "zzzzz.to_csv(\"ZLogisticRegression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzzzz = abc.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzzzz.columns = [\"tn\",\"fp\",\"fn\",\"tp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzzzz['tn'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(zzzzz['tn'][0],zzzzz['fp'][0],zzzzz['fn'][0],zzzzz['tp'][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(z).to_csv(\"ZLogisticRegression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.array(roc_auc[2]).to_csv(\"ROC_logistic_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "lw = 2\n",
    "fig = plt.plot(fppp, tppp, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc1)\n",
    "fig = plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "fig = plt.xlim([0.0, 1.0])\n",
    "fig = plt.ylim([0.0, 1.05])\n",
    "fig = plt.xlabel('False Positive Rate')\n",
    "fig = plt.ylabel('True Positive Rate')\n",
    "fig = plt.title('Receiver operating characteristic example')\n",
    "fig = plt.legend(loc=\"lower right\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot AUC ROC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot Confusion Matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix( tn, fp, fn, tp):\n",
    "    z = [[tn,fp],[fn,tp]]\n",
    "\n",
    "    x = [1,0]\n",
    "    y = [1,0]\n",
    "\n",
    "    z_text  = [['True Negative : ' + str(tn),'False Positive : ' + str(fp)],['False Negative : ' + str(fn),'True Positive : ' + str(tp)]]\n",
    "\n",
    "    # set up figure \n",
    "    fig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "    # add title\n",
    "    fig.update_layout(title_text='<i><b>Confusion matrix - Random Forest</b></i>',\n",
    "                     )\n",
    "\n",
    "    # add custom xaxis title\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                            x=0.5,\n",
    "                            y=-0.15,\n",
    "                            showarrow=False,\n",
    "                            text=\"Predicted value\",\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "\n",
    "    # add custom yaxis title\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                            x=-0.1,\n",
    "                            y=0.5,\n",
    "                            showarrow=False,\n",
    "                            text=\"Real value\",\n",
    "                            textangle=-90,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"))\n",
    "\n",
    "    # adjust margins to make room for yaxis title\n",
    "    fig.update_layout(margin=dict(t=50, l=200))\n",
    "\n",
    "    # add colorbar\n",
    "    fig['data'][0]['showscale'] = True\n",
    "    fig.show()\n",
    "    a = fig.to_json()\n",
    "    return json.loads(json.dumps(a,cls=plotly.utils.PlotlyJSONEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pandas.read_csv(\"data/train.csv\")\n",
    "new_df = grouping_and_onehot(train_data)\n",
    "X_train_res, y_train_res,df_new = over_sampling(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "model = RandomForestClassifier(max_depth=200, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'average_precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e8fafc7e6e60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maverage_precision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'average_precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_res, y_train_res)\n",
    "predicted_labels = model.predict(X_test)\n",
    "average_precision = average_precision_score(y_test, predicted_labels)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "\n",
    "fpr[2], tpr[2], _ = roc_curve(y_test, predicted_labels)\n",
    "roc_auc[2] = auc(fpr[2], tpr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fppp = pandas.DataFrame(fpr[2]).to_csv(\"FPRRandomForestClassifier.csv\")\n",
    "tppp = pandas.DataFrame(tpr[2]).to_csv(\"TPRRandomForestClassifier.csv\")\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "abc = pandas.DataFrame([tn,fp,fn,tp], index=[\"tn\",\"fp\",\"fn\",\"tp\"])\n",
    "zzzzz = abc.transpose()\n",
    "zzzzz.to_csv(\"ZRandomForestClassifier.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "predicted_labels = model.predict(X_test)\n",
    "average_precision = average_precision_score(y_test, predicted_labels)\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "\n",
    "fpr[2], tpr[2], _ = roc_curve(y_test, predicted_labels)\n",
    "roc_auc[2] = auc(fpr[2], tpr[2])\n",
    "fppp = pandas.DataFrame(fpr[2]).to_csv(\"FPRKNN.csv\")\n",
    "tppp = pandas.DataFrame(tpr[2]).to_csv(\"TPRKNN.csv\")\n",
    "tn, fp, fn, tp  = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "abc = pandas.DataFrame([tn,fp,fn,tp], index=[\"tn\",\"fp\",\"fn\",\"tp\"])\n",
    "zzzzz = abc.transpose()\n",
    "zzzzz.to_csv(\"ZKNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
